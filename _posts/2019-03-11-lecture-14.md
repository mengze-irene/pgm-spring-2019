---
layout: distill
title: "Lecture 14: Approximate Inference: Markov Chain Monte Carlo"
description: XXX
date: 2019-03-04

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: Ziheng Cai
    url: "#"  # optional URL to the author's homepage
  - name: XXX
    url: "#"
  - name: Zhuoran Zhang
    url: "#"

editors:
  - name: XXX
    url: "#"  # optional URL to the editor's homepage

abstract: >
 -- XXX
 -- 
 -- XXX

---


<!-- Ziheng -->
## Gibbs Sampling

### Definition

Gibbs Sampling is an Markov Chain Monte Carlo algorithm that samples each random variable of a graphical, one at a time. It is a special case of the Metropolis-Hasting algorithm. It is assumed that $P(\mathbb{x})$ is too complex while $P(x_i|x_{-i})$ is tractable to work with.

### Algorithm

Gibbs Sampling:
1. Let $x_1, \cdots, x_n$ be the variables of the graphical model for which we are estimating the distribution
2. Initialize starting values for $x_1,\cdots,x_n$
3. At time step $t$:
   1. Pick an arbitrary ordering of $x_1,\cdots,x_n$ (this can be arbitrary or random)
   2. For each $x_i$ in the order:
      Sample $x_i^{(t)} \sim P(x_i|x_{-i})$, where $x_i$ is updated immediately by $x_i^{(t)}$ (the new value will be used for the next sampling)
4. Repeat until convergence

### How do we compute the conditional probability $P(x_i|x_{-i})$? -- Recall Markov Blankets:

$$
P(x_i|x_{-i}) = P(x_i|MB(x_i))
$$

For a Bayesian Network, the Markov Blanket of $x$ is the set of parents, childen and co-parents.

For a Markov Random Field, the Markov Blanket of $x$ is its immediate neighbors.

### A 2D Example

The following figure illustrates Gibbs Sampling on two variables $(x_1,x_2)=\mathbf{x}$.

<img src="{{ '/assets/img/notes/lecture-14/gibbs-sampling-2d-example.png' | relative_url }}" style="width: 70%; height: auto;display: block;margin-left: auto;margin-right: auto;"/>

On each iteration, we start from the current state $\mathbf{x}^{(t)}$ and $x_1$ is sampled from conditional density $P(x_1|x_2)$, with $x_2$ fixed to $x_2^{(t)}$. Then $x_2$ is sampled from conditional density $P(x_2|x_1)$, with $x_1$ fixed to $x_1^{(t+1)}$. This gives $\mathbb{x}^{t+1} and completes the iteration.

### Topic Models: Collapsed Gibbs Sampling




<!-- Zhuoran -->

## Practical Aspects of MCMC

### How do we know if our proposal is any good? -- Monitor the acceptance rate:

Choosing the proposal $Q(x' \| x)$ is a tradeoff. The 'narrow', low-variance proposals have high acceptance, but may take many iterations to explore $P(x)$ fully because the proposed $x$ are too close. The 'wide', high-variance proposals have the potential to explore much of $P(x)$, but many proposals are rejected which slows down the sampler. 

A good $Q(x' \| x)$ proposes distant samples $x'$ with a sufficiently high acceptance rate.

<img src="{{ '/assets/img/notes/lecture-14/acceptance_rate.png' | relative_url }}" style="width: 70%; height: auto;display: block;margin-left: auto;margin-right: auto;"/>

Acceptance rate is the fraction of samples that MH accepts. A general guideline is proposals should have ~0.5 acceptance rate <d-cite key="muller1991generic"></d-cite>. 

If both $P(x)$ and $Q(x' \| x)$ are Gaussian, the optimal acceptance rate is ~0.45 for D=1 dimension and approaches ~0.23 as D tends to infinity <d-cite key="roberts1997weak"></d-cite>. 


### How do we know if our proposal is any good? -- Autocorrelation function:

MCMC chains always show autocorrelation (AC), because we are using the previous example to define the transition of the next example. (Note: AC means that adjacent samples in time are highly correlated.) We quantify AC with the autocorrelation fucntion of an r.v.x:

$$
R_x(k) = \frac{\sum_{t=1}^{n-k}(x_t-\bar{x})(x_{t+k}-\bar{x})}{\sum_{t=1}^{n-k}(x_t-\bar{x})^2}
$$

The first-order AC $R_x(1)$ can be used to estimate the Sample Size Inflation Factor (SSIF):

$$
s_x = \frac{1+R_x(1)}{1-R_x(1)}
$$

If we took $n$ samples with SSIF $s_x$, then the effective sample size is $n/s_x$. High autocorrelation leads to smaller effective sample size. We wan proposals $Q(x' \| x)$ with low auto correlation.

<img src="{{ '/assets/img/notes/lecture-14/autocorrelation.png' | relative_url }}" style="width: 70%; height: auto;display: block;margin-left: auto;margin-right: auto;"/>

### How do we know when to stop burn-in? -- Plot the sample values vs time

We can monitor convergence by plotting samples (of r.v.s) from multiple MH runs (chains). (Note: In practice, when people do MCMC, they usually start with multiple MCMC chains rather than one MCMC). If the chains are well-mixed (left), they are probably converged. If the chains are poorly-mixed (right), we should continue burn-in.

<img src="{{ '/assets/img/notes/lecture-14/sample_value_time.png' | relative_url }}" style="width: 70%; height: auto;display: block;margin-left: auto;margin-right: auto;"/>

### How do we know when to stop burn-in? -- Plot the log-likelihood vs time

Many graphical models are high-dimensional, so it is hard to visualize all r.v. chains at once. Instead, we can plot the complete log-likelihood vs. time. The complete log-likelihood is an r.v. that depends on all model r.v.s. Generall, the log-likelihood will climb, then eventually plateau.

<img src="{{ '/assets/img/notes/lecture-14/loglikelihood_time.png' | relative_url }}" style="width: 70%; height: auto;display: block;margin-left: auto;margin-right: auto;"/>

## Summary

The key point is that we are going to use an adaptive proposal. And we are going to have choices of further engineered adaptive proposal to be a conditional distribution of a single random variable given the rest. And by using the Markov Blanket concept, we can make that simple proposal eqsy to manpulate, and get a constant 1 acceptant rate. So that the samples can be better used. We need to take care of convegence rate, good mixing, etc. 

In summary:
 
  * Markov Chain Monte Carlo methods use adaptive proposals $Q(x' \| x)$ to
sample from the true distribution $P(x)$.
  * Metropolis-Hastings allows you to specify any proposal $Q(xâ€™ \| x)$. Though chooing a good $Q(x' \| x)$ requires care.
  * Gibbs sampling sets the proposal $Q(x' \| x)$ to the conditional distribution $P(x' \| x)$:
     1. Acceptance rate is always 1!
     2. But remember that high acceptance usually entails slow exploration
     3. In fact, there are better MCMC algorithms for certain models
  * Knowing when to halt burn-in is an art.




