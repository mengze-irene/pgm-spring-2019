---
layout: distill
title: "Lecture 14: Approximate Inference: Markov Chain Monte Carlo"
description: XXX
date: 2019-03-04

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: XXX
    url: "#"  # optional URL to the author's homepage
  - name: XXX
    url: "#"
  - name: Zhuoran Zhang
    url: "#"

editors:
  - name: XXX
    url: "#"  # optional URL to the editor's homepage

abstract: >
 -- XXX
 -- 
 -- XXX

---

<!-- Zhuoran -->

## Practical Aspects of MCMC

### How do we know if our proposal is any good? -- Monitor the acceptance rate:

Choosing the proposal $Q(x' \| x)$ is a tradeoff. The 'narrow', low-variance proposals have high acceptance, but may take many iterations to explore $P(x)$ fully because the proposed $x$ are too close. The 'wide', high-variance proposals have the potential to explore much of $P(x)$, but many proposals are rejected which slows down the sampler. 

A good $Q(x' \| x)$ proposes distant samples $x'$ with a sufficiently high acceptance rate.

<img src="{{ '/assets/img/notes/lecture-14/acceptance_rate.png' | relative_url }}" style="width: 70%; height: auto;display: block;margin-left: auto;margin-right: auto;"/>

Acceptance rate is the fraction of samples that MH accepts. A general guideline is proposals should have ~0.5 acceptance rate <d-cite key="muller1991generic"></d-cite>. 

If both $P(x)$ and $Q(x' \| x)$ are Gaussian, the optimal acceptance rate is ~0.45 for D=1 dimension and approaches ~0.23 as D tends to infinity <d-cite key="roberts1997weak"></d-cite>. 


### How do we know if our proposal is any good? -- Autocorrelation function:

MCMC chains always show autocorrelation (AC), because we are using the previous example to define the transition of the next example. (Note: AC means that adjacent samples in time are highly correlated.) We quantify AC with the autocorrelation fucntion of an r.v.x:

$$
R_x(k) = \frac{\sum_{t=1}^{n-k}(x_t-\bar{x})(x_{t+k}-\bar{x})}{\sum_{t=1}^{n-k}(x_t-\bar{x})^2}
$$

The first-order AC $R_x(1)$ can be used to estimate the Sample Size Inflation Factor (SSIF):

$$
s_x = \frac{1+R_x(1)}{1-R_x(1)}
$$

If we took $n$ samples with SSIF $s_x$, then the effective sample size is $n/s_x$. High autocorrelation leads to smaller effective sample size. We wan proposals $Q(x' \| x)$ with low auto correlation.

<img src="{{ '/assets/img/notes/lecture-14/autocorrelation.png' | relative_url }}" style="width: 70%; height: auto;display: block;margin-left: auto;margin-right: auto;"/>

### How do we know when to stop burn-in? -- Plot the sample values vs time

We can monitor convergence by plotting samples (of r.v.s) from multiple MH runs (chains). (Note: In practice, when people do MCMC, they usually start with multiple MCMC chains rather than one MCMC). If the chains are well-mixed (left), they are probably converged. If the chains are poorly-mixed (right), we should continue burn-in.

<img src="{{ '/assets/img/notes/lecture-14/sample_value_time.png' | relative_url }}" style="width: 70%; height: auto;display: block;margin-left: auto;margin-right: auto;"/>

### How do we know when to stop burn-in? -- Plot the log-likelihood vs time

Many graphical models are high-dimensional, so it is hard to visualize all r.v. chains at once. Instead, we can plot the complete log-likelihood vs. time. The complete log-likelihood is an r.v. that depends on all model r.v.s. Generall, the log-likelihood will climb, then eventually plateau.

<img src="{{ '/assets/img/notes/lecture-14/loglikelihood_time.png' | relative_url }}" style="width: 70%; height: auto;display: block;margin-left: auto;margin-right: auto;"/>

## Summary

The key point is that we are going to use an adaptive proposal. And we are going to have choices of further engineered adaptive proposal to be a conditional distribution of a single random variable given the rest. And by using the Markov Blanket concept, we can make that simple proposal eqsy to manpulate, and get a constant 1 acceptant rate. So that the samples can be better used. We need to take care of convegence rate, good mixing, etc. 

In summary:
 
  * Markov Chain Monte Carlo methods use adaptive proposals $Q(x' \| x)$ to
sample from the true distribution $P(x)$.
  * Metropolis-Hastings allows you to specify any proposal $Q(xâ€™ \| x)$. Though chooing a good $Q(x' \| x)$ requires care.
  * Gibbs sampling sets the proposal $Q(x' \| x)$ to the conditional distribution $P(x' \| x)$:
     1. Acceptance rate is always 1!
     2. But remember that high acceptance usually entails slow exploration
     3. In fact, there are better MCMC algorithms for certain models
  * Knowing when to halt burn-in is an art.




