---
layout: distill
title: "Lecture 14: Approximate Inference: Markov Chain Monte Carlo"
description: XXX
date: 2019-03-04

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: XXX
    url: "#"  # optional URL to the author's homepage
  - name: Jiacheng Zhu
    url: "#"
  - name: Zhuoran Zhang
    url: "#"

editors:
  - name: XXX
    url: "#"  # optional URL to the editor's homepage

abstract: >
 -- XXX
 -- 
 -- XXX

---

<!-- Zhuoran -->

## Practical Aspects of MCMC

### How do we know if our proposal is any good? -- Monitor the acceptance rate:

Choosing the proposal $Q(x' \| x)$ is a tradeoff. The 'narrow', low-variance proposals have high acceptance, but may take many iterations to explore $P(x)$ fully because the proposed $x$ are too close. The 'wide', high-variance proposals have the potential to explore much of $P(x)$, but many proposals are rejected which slows down the sampler. 

A good $Q(x' \| x)$ proposes distant samples $x'$ with a sufficiently high acceptance rate.

<img src="{{ '/assets/img/notes/lecture-14/acceptance_rate.png' | relative_url }}" style="width: 70%; height: auto;display: block;margin-left: auto;margin-right: auto;"/>

Acceptance rate is the fraction of samples that MH accepts. A general guideline is proposals should have ~0.5 acceptance rate <d-cite key="muller1991generic"></d-cite>. 

If both $P(x)$ and $Q(x' \| x)$ are Gaussian, the optimal acceptance rate is ~0.45 for D=1 dimension and approaches ~0.23 as D tends to infinity <d-cite key="roberts1997weak"></d-cite>. 


### How do we know if our proposal is any good? -- Autocorrelation function:

MCMC chains always show autocorrelation (AC), because we are using the previous example to define the transition of the next example. (Note: AC means that adjacent samples in time are highly correlated.) We quantify AC with the autocorrelation fucntion of an r.v.x:

$$
R_x(k) = \frac{\sum_{t=1}^{n-k}(x_t-\bar{x})(x_{t+k}-\bar{x})}{\sum_{t=1}^{n-k}(x_t-\bar{x})^2}
$$

The first-order AC $R_x(1)$ can be used to estimate the Sample Size Inflation Factor (SSIF):

$$
s_x = \frac{1+R_x(1)}{1-R_x(1)}
$$

If we took $n$ samples with SSIF $s_x$, then the effective sample size is $n/s_x$. High autocorrelation leads to smaller effective sample size. We wan proposals $Q(x' \| x)$ with low auto correlation.

<img src="{{ '/assets/img/notes/lecture-14/autocorrelation.png' | relative_url }}" style="width: 70%; height: auto;display: block;margin-left: auto;margin-right: auto;"/>

### How do we know when to stop burn-in? -- Plot the sample values vs time

We can monitor convergence by plotting samples (of r.v.s) from multiple MH runs (chains). (Note: In practice, when people do MCMC, they usually start with multiple MCMC chains rather than one MCMC). If the chains are well-mixed (left), they are probably converged. If the chains are poorly-mixed (right), we should continue burn-in.

<img src="{{ '/assets/img/notes/lecture-14/sample_value_time.png' | relative_url }}" style="width: 70%; height: auto;display: block;margin-left: auto;margin-right: auto;"/>

### How do we know when to stop burn-in? -- Plot the log-likelihood vs time

Many graphical models are high-dimensional, so it is hard to visualize all r.v. chains at once. Instead, we can plot the complete log-likelihood vs. time. The complete log-likelihood is an r.v. that depends on all model r.v.s. Generall, the log-likelihood will climb, then eventually plateau.

<img src="{{ '/assets/img/notes/lecture-14/loglikelihood_time.png' | relative_url }}" style="width: 70%; height: auto;display: block;margin-left: auto;margin-right: auto;"/>

## Summary

The key point is that we are going to use an adaptive proposal. And we are going to have choices of further engineered adaptive proposal to be a conditional distribution of a single random variable given the rest. And by using the Markov Blanket concept, we can make that simple proposal eqsy to manpulate, and get a constant 1 acceptant rate. So that the samples can be better used. We need to take care of convegence rate, good mixing, etc. 

In summary:
 
  * Markov Chain Monte Carlo methods use adaptive proposals $Q(x' \| x)$ to
sample from the true distribution $P(x)$.
  * Metropolis-Hastings allows you to specify any proposal $Q(xâ€™ \| x)$. Though chooing a good $Q(x' \| x)$ requires care.
  * Gibbs sampling sets the proposal $Q(x' \| x)$ to the conditional distribution $P(x' \| x)$:
     1. Acceptance rate is always 1!
     2. But remember that high acceptance usually entails slow exploration
     3. In fact, there are better MCMC algorithms for certain models
  * Knowing when to halt burn-in is an art.





<!-- Jiacheng Zhu -->

## Optimization in MCMC: Introduction
One of the struggle people had in all vanilla MCMC methods is so called random walk behavior, which is caused by the proposed 
distribution. However, we want to propose prefered biased samples. How to impose the derivative (maybe likelihood function) into the proposal in a mathematically elegent fashion had became an important question

### Hamiltonian Monte Carlo
Hamiltonian Dynamics comes form physics, is given by

$$
H(p,x) = U(x) + K(p)
$$

Where $x$ is the position vector, $p$ is the momentum vector. $U(x)$ is the potential energy and $K(p)$ stands for kinetic energy. There are many interesting connections betrween the terms and derivatives over Hamiltonian. One of the key of Hamiltonian is that 

$$
\frac{d x_i}{dt} = \frac{\partial H}{\partial p_i}
$$

$$
\frac{d p_i}{dt} = - \frac{\partial H}{\partial x_i}
$$

When we want to sample a target distribution, we can leverage on gradient methods by introducing more variables to an auxiliary distribution. 

$$
P_H (x,p) = \frac{e^{-E(x)-K(p)}}{Z_h}
$$
Thus, using Hamiltonian, we are able to define the change of state v.s. the gradient of a loss function over the change.

### How to update: Euler's Method
There are multiple way to compute the $\delta$ in the state as a function of teh gradient.
The Euler's Method directly estabilsh the change in $p$ (momentum), and $q$ (position) as a function of the loss. 

$$
p_i(t + \epsilon) = p_i(t) + \epsilon \frac{dp_i}{dt}(t) = p_i(t) - \epsilon \frac{\partial U}{\partial q_i}(q(t))
$$

$$
q_i(t + \epsilon) = q_i(t) + \epsilon \frac{dq_i}{dt}(t) = q_i(t) + \epsilon \frac{ p_i(t) }{m_i }
$$

### How to update: Leapfrog Method
Leapfrog Method is prefered, because it alternates between the $p$ and $q$ to calculate the updates in a very controlled fashion. So behaviors like over shooting and under shooting can be avoided.
$$
p_i(t + \epsilon /2) = p_i(t) - (\epsilon /2) \frac{\partial U}{\partial q_i}(q(t))
$$

$$
q_i(t + \epsilon) = q_i(t) + \epsilon \frac{p_i(t + \epsilon /2)}{m_i}
$$

$$
p_i(t + \epsilon) = p_i(t + \epsilon /2) - (\epsilon / 2) \frac{\partial U}{\partial q_i}(q(t + \epsilon))
$$

<img src="{{ '/assets/img/notes/lecture-14/compare_e_l.png' | relative_url }}" style="width: 70%; height: auto;display: block;margin-left: auto;margin-right: auto;"/>

### MCMC from Hamiltonian Dynamics
Let $q$ be variable of interest, $p$ is introduced as an auxiliary random variable in order to define the Hamiltonian. 

$$
P(q,p)=\frac{1}{Z} \exp (-U(q)/T) \exp(-K(p)/TY)
$$

Where $U(q) = -log [\pi (q) L(q\|D)]$ and $K(p) = \sum^d_{i=1} \frac{p^2_i}{2m_i}$. Here it is a Bayesian setting where we have both the distribution of hidden states or the states of interest and also conditioned from priors.
$U(q) = -log [\pi (q) L(q\|D)]$ connects to the likelihhod, the gradient of which is not directly involved in the proposal of next $q$. Then a accept/ reject critera is built based on the change of the Hamiltonian.  

<img src="{{ '/assets/img/notes/lecture-14/sudo.png' | relative_url }}" style="width: 60%; height: auto;display: block;margin-left: auto;margin-right: auto;"/>

### Langevin Dynamics
Langevin Dynamics is special case of Hamiltonian. Instead of doing Leapfrog, Langevin does a more sophiscated update based on second-order updates of the sampling states.

$$
q^*_i = q_i - \frac{\epsilon^2}{2} \frac{\partial U}{ \partial q_i}(q) + \epsilon p_i
$$

$$
p^*_i = p_i - \frac{\epsilon}{2} \frac{\partial U}{ \partial q_i}(q) - \frac{\epsilon}{2} \frac{\partial U}{ \partial q_i}(q^*)
$$
Even for a strange distribution with constrains on regions, this augmented optimization methods still deal with it.



## Optimization in MCMC: Conclusion

  * Using Hamiltonian, we are able to define the change of state v.s. the gradient of a loss function over the change.
  * Hamiltonian Mento Carlo can improve acceptence rate and give better mixing by incorporating optimization based approaches to generate better proposals.
  * Stochastic variants can be used to improve performance in large dataset scenarios.
  * Hamiltonnian Mento Carlo may not be used for discrete variable


