---
layout: distill
title: Lecture 14 Markov Chain Monte Carlo 
description: An example of a distill-style lecture notes that showcases the main elements.
date: 2019-03-11

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: Irene Li
    url: "#"  # optional URL to the author's homepage
  - name: George Cai
    url: "#"
  - name: Zhuoran Zhang
    url: "#"
  - name: Jiacheng Zhu

editors:
  - name: Editor 1  # editor's full name
    url: "#"  # optional URL to the editor's homepage

abstract: >
  An example abstract block.
---

<!-- Irene -->

## Recap of Monte Carlo
### Monte Carlo methods are algorithms that:
* Generate samples from a given probability distribution $p(x)$.
* Estimate expectations of functions $E[f(x)]$ under a distribution $p(x)$.

### Why is Monte Carlo useful?
* Can use samples of $p(x)$ to approximate $p(x)$ itself 
  * Allow us to do graphical model inference when we can't compute $p(x)$.
* Expectations $E[f(x)]$ reveal interesting properties about $p(x)$, e.g., means and variances.

### Limitations of Monte Carlo
* Direct sampling
  * Hard to get rare events in high-dimensional spaces.
  * Infeasible for MRFs, unless we know the normalizer $Z$.
* Rejection sampling, Importance sampling
  * Do not work well if the proposal $Q(x)$ is very different from $P(x)$.
  * Yet constructing a $Q(x)$ similar to $P(x)$ can be difficult.
    * Requires knowledge of the analytical form of $P(x)$ - but if we had that, we wouldn't even need to sample!
* Intuition: Instead of a fixed proposal $Q(x)$, use an adaptive proposal.

## Markov Chain Monte Carlo (MCMC)
MCMC algorithms feature adaptive proposals.
* Instead of $Q(x^\prime)$, use $Q(x^\prime \mid x)$ where $x^\prime$ is the new state being sampled, and $x$ is the previous sample.
* As $x$ changes, $Q(x^\prime \mid x)$ can also change (as a function of $x^\prime$).

<figure>
<img src="{{ '/assets/img/notes/lecture-14/MCMC.png' | relative_url }}" />
<figcaption>
Comparison between using a fixed (bad) proposal and an adaptive proposal.
</figcaption>
</figure>

To understand how MCMC works, we need to look at Markov Chains first.

### Markov Chains
* A Markov Chain is a sequence of random variables $x^{(1)}$, $x^{(2)}$, ..., $x^{(n)}$ with the Markov Property

$$
P(x^{(n)}=x\ |x^{(1)}, ..., x^{(n-1)})=P(x^{(n)}=x\ |x^{(n-1)})
$$
  * $P(x^{(n)}=x\mid x^{(n-1)})$ is known as the **transition kernel**.
  * The next state depends only on the preceding states.
  * Random variables $x^{(i)}$ can be **vectors**.
    * We define $x^{(i)}$ to be the t-th sample of **all** variables in a graphical model
    * $x^{(i)}$ represents the entire state of the graphical model at time $t$.

* We study homogeneous Markov Chains, in which the transition kernel $P(x^{(n)}=x\mid x^{(n-1)})$ is fixed with time.
  * To emphasize this, we will call the kernel $T(x^\prime\mid x)$, where $x$ is the previous state and $x^\prime$ is the next state.
   
### Markov Chains Concepts
Define a few important concepts of Markov Chains(MC)
* **Probability distribution over states**: $\pi^{(t)}(x)$ is a distribution over the state of the system $x$, at time $t$.
  * When dealing with MCs, we don't think of the system as being in one state, but as having a distribution over states.
  * For graphical models, remember that $x$ represents **all** variables.
* **Transitions**: recall that states transition from $x^{(t)}$ to $x^{(t+1)}$ according to the transition kernel $T(x^\prime\mid  x)$.
  * We can also transition entire distributions: 
    $$\pi^{(t+1)}(x')=\sum_{x} \pi^{(t)}(x)T(x' \mid x)$$
  * At time t, state $x$ has probability mass $\pi^{(t)}(x)$. The transition probability redistributes this mass to other states $x^\prime$.
* **Stationary distributions**: $\pi^{(t)}(x)$ is stationary if it does not change under the transition kernel:
$\pi(x^\prime)=\sum_{x} \pi(x)T(x^\prime\mid x)$, for all $x^\prime$. To understand stationary distributions, we need to define some notions:
  * **Irreducible**: an MC is irreducible if you can get from any state $x$ to any other state $x^\prime$ with probability > 0 in a finite number of steps, i.e., there are no unreachabble parts of the state space.
  * **Aperiodic**: an MC is aperiodic if you can return to any state x at any time.
    * Periodic MCs have states that need ≥2 time steps to return to (cycles).
  * **Ergodic (or regular)**: an MC is ergodic if it is irreducible and aperiodic
    * Ergodicity is important: it implies you can reach the stationary distribution $\pi_{st}(x)$, no matter the initial distribution $\pi^{(0)}(x)$.
    * All good MCMC algorithms must satisfy ergodicity, so that you can’t initialize in a way that will never converge.
  * **Reversible (detailed balance)**: an MC is reversible if there exists a distribution $\pi(x)$ such that the detailed balance condition is satisfied:
  $$\pi(x')T(x\ |x')=\pi(x)T(x'\ |x)$$
    Probability of $x^\prime ->x$ is the same as $x->x^\prime$.
      * Reversible MCs **always** have a stationary distribution! Proof:
      <d-math block>
      \begin{aligned}
      \pi(x')T(x\ |x') & = \pi(x)T(x'\ | x) \\
      \sum_{x}\pi(x')T(x\ |x') & = \sum_{x}\pi(x)T(x'\ | x) \\
      \pi(x')\sum_{x}T(x\ |x') & = \sum_{x}\pi(x)T(x'\ | x) \\
      \pi(x') & = \sum_{x}\pi(x)T(x'\ | x) 
      \end{aligned}
      </d-math>
      Note that the last line is the definition of a stationary distribution!

## Metropolis-Hastings (MH) -- An MCMC method
### How the MH algorithm works in practice
1. Draws a sample $x^\prime$ from $Q(x^\prime\mid x)$, where x is the previous sample.
2. The new sample $x^\prime$ is accepted with some probability $A(x^\prime \mid x)=min(1, \frac{P(x^\prime)Q(x\mid x^\prime)}{P(x)Q(x^\prime\mid x)})$
  * $A(x^\prime\mid x)$ is like a ratio of importance sampling weights
    * $P(x^\prime)/Q(x^\prime\mid x)$ is the importance weight for $x^\prime$, $P(x)/Q(x\mid x^\prime)$ is the importance weight for $x$.
    * We devide the importance wieght for $x^\prime$ by that of $x$.
    * Notice that we only need to compute $P(x^\prime)/P(x)$ rather than $P(x^\prime)$ or $P(x)$ separately, so we don't need to know the normalizer.
    * $A(x^\prime\mid x)$ ensures that, after sufficiently many draws, our samples will come from the true distribution $P(x)$.
 
<figure>
<img src="{{ '/assets/img/notes/lecture-14/MH_algo.png' | relative_url }}" />
<figcaption>
The Metropolis-Hastings Algorithm
</figcaption>
</figure>
 

### Why does Metropolis-Hastings work?
Since we draw a sample $x^\prime$ according to $Q(x^\prime\mid x)$, and then accept/reject according to $A(x^\prime\mid x)$, the transition kernel is:
$$T(x'\mid x)=Q(x'\mid x)A(x'\mid x)$$

We can prove that MH satisfies detailed balance (or reversibility):

Recall that $A(x^\prime\mid x)=min(1, \frac{P(x^\prime)Q(x\mid x^\prime)}{P(x)Q(x^\prime\mid x)})$, which implies the following:

$$\text{If } A(x^\prime\mid x)<1 \text{ then } \frac{P(x^\prime)Q(x\mid x^\prime)}{P(x)Q(x^\prime\mid x)}>1 \text{ and thus } A(x\mid x^\prime)=1$$

Now suppose $A(x^\prime \mid x)<1$ and $A(x \mid x^\prime)=1$. We have:
<d-math block>
\begin{aligned}
A(x'\ |x) & = \frac{P(x')Q(x\ |x')}{P(x)Q(x'\ |x)} \\
P(x)Q(x'\ |x)A(x'\ |x) & = P(x')Q(x\ |x') \\
P(x)Q(x'\ |x)A(x'\ |x) & =P(x')Q(x\ |x')A(x\ |x') \\
\pi(x')T(x\ |x')& =\pi(x)T(x'\ |x)
\end{aligned}
</d-math>
The last line is exactly the detailed balance condition. 
In other words, the MH algorithm leads to a stationary distribution $P(x)$. Recall we defined $P(x)$ to be the true distribution of $x$. Thus, the MH algorithm eventually converges to the true distribution!

### Caveats
Although MH eventually converges to the true distribution $P(x)$, we have no guarantees as to when this will occur.
* MH has a "burn-in" period: an initial number of samples are thrown away because they are not from the true distribution.   
  * The burn-in period represents the un-converged part of the Markov Chain.
  * Knowing when to halt burn-in is an art. We will look at some techniques later in this lecture.

