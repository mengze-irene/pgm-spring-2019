---
layout: distill
title: "Lecture 14: Approximate Inference: Markov Chain Monte Carlo"
description: XXX
date: 2019-03-04

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: Irene Li
    url: "#"  # optional URL to the author's homepage
  - name: Ziheng Cai
    url: "#"
  - name: Zhuoran Zhang
    url: "#"

editors:
  - name: XXX
    url: "#"  # optional URL to the editor's homepage

abstract: >
 -- XXX
 -- 
 -- XXX

---

<!-- Irene -->

## Recap of Monte Carlo
### Monte Carlo methods are algorithms that:
* Generate samples from a given probability distribution $p(x)$.
* Estimate expectations of functions $E[f(x)]$ under a distribution $p(x)$.

### Why is Monte Carlo useful?
* Can use samples of $p(x)$ to approximate p(x) itself 
  * Allow us to do graphical model inference when we can't compute $p(x)$.
* Expectations $E[f(x)]$ reveal interesting properties about $p(x)$, e.g., means and variances of $p(x)$.

### Limitations of Monte Carlo
* Direct sampling
  * Hard to get rare events in high-dimensional spaces
  * Infeasible for MRFs, unless we know the normalizer $Z$.
* Rejection sampling, Importance sampling
  * Do not work well if the proposal $Q(x)$ is very different from $P(x)$.
  * Yet constructing a $Q(x)$ similar to $P(x)$ can be difficult.
    * Requires knowledge of the analytical form of $P(x)$ - but if we had that, we wouldn't even need to sample!
* Intuition: Instead of a fixed proposal $Q(x)$, use an adaptive proposal.

## Markov Chain Monte Carlo (MCMC)
MCMC algorithms feature adaptive proposals.
* Instead of $Q(x\prime)$, use $Q(x\prime\ |x)$ where $x'$ is the new state being sampled, and $x$ is the previous sample.
* As $x$ changes, $Q(x\prime\ |x)$ can also change (as a function of $x'$).

<figure>
<img src="{{ '/assets/img/notes/lecture-14/MCMC.png' | relative_url }}" />
<figcaption>
Comparison between using a fixed (bad) proposal and an adaptive proposal.
</figcaption>
</figure>

To understand how MCMC works, we need to look at Markov Chains first.

### Markov Chains
* A Markov Chain is a sequence of random variables $x^{(1)}$, $x^{(2)}$, ..., $x^{(n)}$ with the Markov Property

$$
P(x^{(n)}=x\ |x^{(1)}, ..., x^{(n-1)})=P(x^{(n)}=x\ |x^{(n-1)})
$$
  * $P(x^{(n)}=x\ |x^{(n-1)})$ is known as the **transition kernel**.
  * The next state depends only on the preceding states.
  * Random variables $x^{(i)}$ can be **vectors**.
    * We define $x^{(i)}$ to be the t-th sample of **all** variables in a graphical model
    * $x^{(i)}$ represents the entire state of the graphical model at time $t$.

* We study homogeneous Markov Chains, in which the transition kernel $P(x^{(n)}=x\ |x^{(n-1)})$ is fixed with time.
  * To emphasize this, we will call the kernel $T(x'\ |x)$, where $x$ is the previous state and $x'$ is the next state.
   
### Markov Chains Concepts
Define a few important concepts of Markov Chains(MC)
* **Probability distribution over states**: $\pi^{(t)}(x)$ is a distribution over the state of the system $x$, at time $t$.
  * When dealing with MCs, we don't think of the system as being in one state, but as having a distribution over states.
  * For graphical models, remember that $x$ represents **all** variables.
* **Transitions**: recall that states transition from $x^{(t)}$ to $x^{(t+1)}$ according to the transition kernel $T(x'\ |x)$.
  * We can also transition entire distributions: 
    $$\pi^{(t+1)}(x')=\sum_{x} \pi^{(t)}(x)T(x'\ |x)$$
  * At time t, state x has probability mass $\pi^{(t)}(x)$. The transition probability redistributes this mass to other states $x’$.
* **Stationary distributions**: $\pi^{(t)}(x)$ is stationary if it does not change under the transition kernel:
$\pi(x')=\sum_{x} \pi(x)T(x'\ |x)$, for all $x'$. To understand stationary distributions, we need to define some notions:
  * **Irreducible**: an MC is irreducible if you can get from any state x to any other state x’ with probability > 0 in a finite number of steps, i.e., there are no unreachabble parts of the state space.
  * **Aperiodic**: an MC is aperiodic if you can return to any state x at any time.
    * Periodic MCs have states that need ≥2 time steps to return to (cycles).
  * **Ergodic (or regular)**: an MC is ergodic if it is irreducible and aperiodic
    * Ergodicity is important: it implies you can reach the stationary distribution $\pi_{st}(x)$, no matter the initial distribution $\pi^{(0)}(x)$.
    * All good MCMC algorithms must satisfy ergodicity, so that you can’t initialize in a way that will never converge.
  * **Reversible (detailed balance)**: an MC is reversible if there exists a distribution $\pi(x)$ such that the detailed balance condition is satisfied:
  $$\pi(x')T(x\ |x')=\pi(x)T(x'\ |x)$$
    Probability of $x'->x$ is the same as $x->x'$.
      * Reversible MCs **always** have a stationary distribution! Proof:
      <d-math block>
      \begin{aligned}
      \pi(x')T(x\ |x') & = \pi(x)T(x'\ | x) \\
      \sum_{x}\pi(x')T(x\ |x') & = \sum_{x}\pi(x)T(x'\ | x) \\
      \pi(x')\sum_{x}T(x\ |x') & = \sum_{x}\pi(x)T(x'\ | x) \\
      \pi(x') & = \sum_{x}\pi(x)T(x'\ | x) 
      \end{aligned}
      </d-math>
      Note that the last line is the definition of a stationary distribution!

## Metropolis-Hastings (MH) -- An MCMC method
### How the MH algorithm works in practice
1. Draws a sample $x'$ from $Q(x'\ |x)$, where x is the previous sample.
2. The new sample $x’$ is accepted or rejected with some probability $A(x'\ | x)$
  * This acceptance probability is $A(x'\ | x)=min(1, \frac{P(x')Q(x\ |x')}{P(x)Q(x'\ |x)})$
  * $A(x'\ | x)$ is like a ratio of importance sampling weights
    * $P(x')/Q(x'\ |x)$ is the importance weight for $x'$, $P(x)/Q(x\ |x')$ is the mportance weight for $x$.
    * We devide the importance wieght for $x'$ by that of $x$.
    * Notice that we only need to compute $P(x')/P(x)$ rather than $P(x')$ or $P(x)$ separately, so we don't need to know the normalizer.
  * $A(x'\ | x)$ ensures that, after sufficiently many draws, our samples will come from the true distribution $P(x)$.
 
<figure>
<img src="{{ '/assets/img/notes/lecture-14/MH_algo.png' | relative_url }}" />
<figcaption>
The Metropolis-Hastings Algorithm
</figcaption>
</figure>
 

### Why does Metropolis-Hastings work?
Since we draw a sample x' according to $Q(x'\ |x)$, and then accept/reject according to $A(x'\ |x)$, the transition kernel is:
$$T(x'\ |x)=Q(x'\ | x)A(x'\ |x)$$
We can prove that MH satisfies detailed balance/reversibility:
Recall that 
$$A(x'\ | x)=min(1, \frac{P(x')Q(x\ |x')}{P(x)Q(x'\ |x)})$$.

This implies the following:
$$\text{if} A(x'\ |x)<1 \text{then} \frac{P(x')Q(x\ |x')}{P(x)Q(x'\ |x)}>1 \text{and thus} A(x\ |x')=1$$

Now suppose $A(x'\ |x)<1$ and $A(x \ | x')=1$. We have:
<d-math block>
\begin{aligned}
A(x'\ |x) & = \frac{P(x')Q(x\ |x')}{P(x)Q(x'\ |x)} \\
P(x)Q(x'\ |x)A(x'\ |x) & = P(x')Q(x\ |x') \\
P(x)Q(x'\ |x)A(x'\ |x) & =P(x')Q(x\ |x')A(x\ |x') \\
\pi(x')T(x\ |x')& =\pi(x)T(x'\ |x)
\end{aligned}
</d-math>
The last line is exactly the detailed balance condition. 
In other words, the MH algorithm leads to a stationary distribution $P(x)$. Recall we defined $P(x)$ to be the true distribution of x. Thus, the MH algorithm eventually converges to the true distribution!

### Caveats
Although MH eventually converges to the true distribution $P(x)$, we have no guarantees as to when this will occur.
* MH has a "burn-in" period: an initial number of samples are thrown away because they are not from the true distribution.   
  * The burn-in period represents the un-converged part of the Markov Chain.
  * Knowing when to halt burn-in is an art. We will look at some techniques later in this lecture.

<!-- Ziheng -->
## Gibbs Sampling

### Definition

Gibbs Sampling is an Markov Chain Monte Carlo algorithm that samples each random variable of a graphical, one at a time. It is a special case of the Metropolis-Hasting algorithm. It is assumed that $P(\mathbb{x})$ is too complex while $P(x_i\|x_{-i})$ is tractable to work with.

### Algorithm

Gibbs Sampling:
1. Let $x_1, \cdots, x_n$ be the variables of the graphical model for which we are estimating the distribution
2. Initialize starting values for $x_1,\cdots,x_n$
3. At time step $t$:
   1. Pick an arbitrary ordering of $x_1,\cdots,x_n$ (this can be arbitrary or random)
   2. For each $x_i$ in the order:
      Sample $x_i^{(t)} \sim P(x_i|x_{-i})$, where $x_i$ is updated immediately by $x_i^{(t)}$ (the new value will be used for the next sampling)
4. Repeat until convergence

### How do we compute the conditional probability $P(x_i|x_{-i})$? -- Recall Markov Blankets:

$$
P(x_i|x_{-i}) = P(x_i|MB(x_i))
$$

For a Bayesian Network, the Markov Blanket of $x$ is the set of parents, childen and co-parents.

For a Markov Random Field, the Markov Blanket of $x$ is its immediate neighbors.

### A 2D Example

The following figure illustrates Gibbs Sampling on two variables $(x_1,x_2)=\mathbf{x}$.

<img src="{{ '/assets/img/notes/lecture-14/gibbs-sampling-2d-example.png' | relative_url }}" style="width: 70%; height: auto;display: block;margin-left: auto;margin-right: auto;"/>

On each iteration, we start from the current state $\mathbf{x}^{(t)}$ and $x_1$ is sampled from conditional density $P(x_1\|x_2)$, with $x_2$ fixed to $x_2^{(t)}$. Then $x_2$ is sampled from conditional density $P(x_2\|x_1)$, with $x_1$ fixed to $x_1^{(t+1)}$. This gives $\mathbb{x}^{t+1} and completes the iteration.

### Topic Models: Collapsed Gibbs Sampling




<!-- Zhuoran -->

## Practical Aspects of MCMC

### How do we know if our proposal is any good? -- Monitor the acceptance rate:

Choosing the proposal $Q(x' \| x)$ is a tradeoff. The 'narrow', low-variance proposals have high acceptance, but may take many iterations to explore $P(x)$ fully because the proposed $x$ are too close. The 'wide', high-variance proposals have the potential to explore much of $P(x)$, but many proposals are rejected which slows down the sampler. 

A good $Q(x' \| x)$ proposes distant samples $x'$ with a sufficiently high acceptance rate.

<img src="{{ '/assets/img/notes/lecture-14/acceptance_rate.png' | relative_url }}" style="width: 70%; height: auto;display: block;margin-left: auto;margin-right: auto;"/>

Acceptance rate is the fraction of samples that MH accepts. A general guideline is proposals should have ~0.5 acceptance rate <d-cite key="muller1991generic"></d-cite>. 

If both $P(x)$ and $Q(x' \| x)$ are Gaussian, the optimal acceptance rate is ~0.45 for D=1 dimension and approaches ~0.23 as D tends to infinity <d-cite key="roberts1997weak"></d-cite>. 


### How do we know if our proposal is any good? -- Autocorrelation function:

MCMC chains always show autocorrelation (AC), because we are using the previous example to define the transition of the next example. (Note: AC means that adjacent samples in time are highly correlated.) We quantify AC with the autocorrelation fucntion of an r.v.x:

$$
R_x(k) = \frac{\sum_{t=1}^{n-k}(x_t-\bar{x})(x_{t+k}-\bar{x})}{\sum_{t=1}^{n-k}(x_t-\bar{x})^2}
$$

The first-order AC $R_x(1)$ can be used to estimate the Sample Size Inflation Factor (SSIF):

$$
s_x = \frac{1+R_x(1)}{1-R_x(1)}
$$

If we took $n$ samples with SSIF $s_x$, then the effective sample size is $n/s_x$. High autocorrelation leads to smaller effective sample size. We wan proposals $Q(x' \| x)$ with low auto correlation.

<img src="{{ '/assets/img/notes/lecture-14/autocorrelation.png' | relative_url }}" style="width: 70%; height: auto;display: block;margin-left: auto;margin-right: auto;"/>

### How do we know when to stop burn-in? -- Plot the sample values vs time

We can monitor convergence by plotting samples (of r.v.s) from multiple MH runs (chains). (Note: In practice, when people do MCMC, they usually start with multiple MCMC chains rather than one MCMC). If the chains are well-mixed (left), they are probably converged. If the chains are poorly-mixed (right), we should continue burn-in.

<img src="{{ '/assets/img/notes/lecture-14/sample_value_time.png' | relative_url }}" style="width: 70%; height: auto;display: block;margin-left: auto;margin-right: auto;"/>

### How do we know when to stop burn-in? -- Plot the log-likelihood vs time

Many graphical models are high-dimensional, so it is hard to visualize all r.v. chains at once. Instead, we can plot the complete log-likelihood vs. time. The complete log-likelihood is an r.v. that depends on all model r.v.s. Generall, the log-likelihood will climb, then eventually plateau.

<img src="{{ '/assets/img/notes/lecture-14/loglikelihood_time.png' | relative_url }}" style="width: 70%; height: auto;display: block;margin-left: auto;margin-right: auto;"/>

## Summary

The key point is that we are going to use an adaptive proposal. And we are going to have choices of further engineered adaptive proposal to be a conditional distribution of a single random variable given the rest. And by using the Markov Blanket concept, we can make that simple proposal eqsy to manpulate, and get a constant 1 acceptant rate. So that the samples can be better used. We need to take care of convegence rate, good mixing, etc. 

In summary:
 
  * Markov Chain Monte Carlo methods use adaptive proposals $Q(x' \| x)$ to
sample from the true distribution $P(x)$.
  * Metropolis-Hastings allows you to specify any proposal $Q(x’ \| x)$. Though chooing a good $Q(x' \| x)$ requires care.
  * Gibbs sampling sets the proposal $Q(x' \| x)$ to the conditional distribution $P(x' \| x)$:
     1. Acceptance rate is always 1!
     2. But remember that high acceptance usually entails slow exploration
     3. In fact, there are better MCMC algorithms for certain models
  * Knowing when to halt burn-in is an art.




